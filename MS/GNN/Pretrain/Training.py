import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch_geometric.nn import DataParallel
from torch_geometric.loader import DataListLoader
from torch.optim.swa_utils import AveragedModel, SWALR
from tqdm import tqdm
import torch.multiprocessing
import os
import re

from .DijkstraGnn import GNNPretrainModel, GLOBAL_STATS, DynamicGraphDataset
from ...Env.NetworkGenerator import TopologyGenerator

# === [æ–°å¢] æ ¸å¿ƒæ­¦å™¨ï¼šFocal Loss ===
class FocalLoss(nn.Module):
  def __init__(self, alpha=0.25, gamma=2.0, logits=True, reduce=True):
    super(FocalLoss, self).__init__()
    self.alpha = alpha
    self.gamma = gamma
    self.logits = logits
    self.reduce = reduce

  def forward(self, inputs, targets):
    if self.logits:
      BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
    else:
      BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
    pt = torch.exp(-BCE_loss)
    F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

    if self.reduce:
      return torch.mean(F_loss)
    else:
      return F_loss

if __name__ == "__main__":
  try:
    torch.multiprocessing.set_start_method('spawn', force=True)
  except RuntimeError: pass
  try:
    torch.multiprocessing.set_sharing_strategy('file_system')
  except RuntimeError: pass

  print("ğŸš€ å¼€å§‹é˜¶æ®µ 1B: GNN ä¸»ä½“é¢„è®­ç»ƒ (ç»ˆæå†²åˆº - Focal Loss + çƒ­é‡å¯)...")

  # --- 1. è¶…å‚æ•°é…ç½® ---
  EPOCHS = 3000          # ä¿æŒ 400ï¼Œé…åˆçƒ­é‡å¯éœ€è¦æ›´å¤šè½®æ¬¡
  GNN_DIM = 256
  NUM_LAYERS = 6
  BATCH_SIZE = 128       
  SAMPLES_PER_EPOCH = 6400
  
  # [è°ƒæ•´] é…åˆçƒ­é‡å¯ï¼Œåˆå§‹ LR å¯ä»¥ç¨å¾®ç»™é«˜ä¸€ç‚¹ç‚¹ï¼Œè®©å®ƒæœ‰èƒ½åŠ›è·³å‡ºå‘
  LEARNING_RATE = 2e-7
  
  NODE_FEAT_DIM = 5
  EDGE_FEAT_DIM = 2

  # [æ–­ç‚¹ç»­è®­] å»ºè®®è®¾ç½®ä¸ºä½ æœ€æ–°çš„æœ€å¥½æ¨¡å‹ï¼Œä¾‹å¦‚ Epoch 290+ çš„
  RESUME_PATH = "./MS/GNN/Pretrain/pretrained_model.pth"

  if torch.cuda.is_available():
    torch.cuda.init()
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print(f"Using device: {device}")

  # --- 2. åˆå§‹åŒ–ç»„ä»¶ ---
  topo_gen = TopologyGenerator(num_nodes_range=(20, 30), m_ba=2)
  model = GNNPretrainModel(NODE_FEAT_DIM, GNN_DIM, EDGE_FEAT_DIM, NUM_LAYERS)
  
  swa_model = AveragedModel(model) # åˆ›å»º SWA æ¨¡å‹å½±å­
  swa_start = 300 # ä»ç¬¬ 300 è½®å¼€å§‹æ”¶é›† SWA æƒé‡
  
  
  start_epoch = 0
  if RESUME_PATH is not None and os.path.exists(RESUME_PATH):
    print(f"ğŸ”„ æ­£åœ¨ä» {RESUME_PATH} åŠ è½½æ£€æŸ¥ç‚¹...")
    state_dict = torch.load(RESUME_PATH, map_location='cpu')
    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
    model.load_state_dict(new_state_dict)
    print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼")
    start_epoch = 300

  model = model.to(device)
  if torch.cuda.device_count() > 1:
    print(f"âœ¨ å¯ç”¨ {torch.cuda.device_count()} å¼  GPU è¿›è¡Œ PyG DataParallel åŠ é€Ÿ")
    model = DataParallel(model)
      
  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5) # [å¾®è°ƒ] åŠ ä¸€ç‚¹ç‚¹ weight_decay é˜²æ­¢è¿‡æ‹Ÿåˆ
  swa_scheduler = SWALR(optimizer, swa_lr=1e-5)
  # [æ ¸å¿ƒå‡çº§ 1] ä½¿ç”¨ä½™å¼¦é€€ç«çƒ­é‡å¯è°ƒåº¦å™¨
  # T_0=50: é¦–æ¬¡é‡å¯å‘¨æœŸä¸º 50 Epoch
  # T_mult=1: ä¹‹åæ¯æ¬¡é‡å¯å‘¨æœŸä¿æŒ 50 Epoch (ä½ å¯ä»¥è®¾ä¸º 2 è®©å‘¨æœŸå˜é•¿)
  # eta_min=1e-6: å­¦ä¹ ç‡æœ€ä½é™åˆ° 1e-6
  from torch.optim import lr_scheduler
  scheduler = lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer, T_0=50, T_mult=2, eta_min=1e-10
  )

  # [æ ¸å¿ƒå‡çº§ 2] ä½¿ç”¨ Focal Loss æ›¿ä»£ BCE
  # alpha=0.85: å¼ºçƒˆå¢åŠ æ­£æ ·æœ¬ï¼ˆæœ€çŸ­è·¯å¾„è¾¹ï¼‰çš„æƒé‡ï¼Œå› ä¸ºå®ƒä»¬å¤ªå°‘äº†
  # gamma=2.0: æ ‡å‡†çš„å›°éš¾æ ·æœ¬èšç„¦å‚æ•°
  loss_fn = FocalLoss(alpha=0.85, gamma=2.0, logits=True)

  best_acc = 0.9975
  # --- 3. è®­ç»ƒå¾ªç¯ ---
  for epoch in range(start_epoch, EPOCHS):
    model.train()
    dataset = DynamicGraphDataset(topo_gen, GLOBAL_STATS, max_samples=SAMPLES_PER_EPOCH)
    train_loader = DataListLoader(dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)

    total_loss = 0.0
    total_acc = 0.0
    num_batches = 0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}", unit="batch")

    for batch_data_list in pbar:
      optimizer.zero_grad()
      edge_logits = model(batch_data_list)
      y_true = torch.cat([data.y for data in batch_data_list]).to(device)
      
      loss = loss_fn(edge_logits, y_true) # ä½¿ç”¨ Focal Loss
      
      loss.backward()
      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
      optimizer.step()

      current_loss = loss.item()
      total_loss += current_loss
      predicted = (edge_logits > 0.0).float()
      current_acc = (predicted == y_true).float().mean().item()
      total_acc += current_acc
      num_batches += 1
      pbar.set_postfix({"Loss": f"{current_loss:.4f}", "Acc": f"{current_acc:.2%}"})

    avg_loss = total_loss / num_batches
    avg_acc = total_acc / num_batches
    current_lr = optimizer.param_groups[0]['lr']
    
    # æ³¨æ„ï¼šCosineAnnealingWarmRestarts éœ€è¦åœ¨æ¯æ¬¡ step() åæ›´æ–°ï¼Œæˆ–è€…æ¯ epoch æ›´æ–°
    # è¿™é‡Œæˆ‘ä»¬åœ¨ epoch ç»“æŸæ—¶æ›´æ–°ã€‚æ³¨æ„å®ƒä¸éœ€è¦ä¼ å…¥éªŒè¯é›† lossã€‚

    scheduler.step(epoch + 1 / EPOCHS) # åŸæ¥çš„è°ƒåº¦å™¨

    print(f"Epoch {epoch+1} å®Œæˆ. Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.2%}, LR: {current_lr:.2e}")

    if avg_acc > best_acc:
      model_to_save = model.module if isinstance(model, DataParallel) else model
      torch.save(model_to_save.state_dict(), RESUME_PATH)
      best_acc = avg_acc

  print("âœ… é˜¶æ®µ 1B é¢„è®­ç»ƒç»ˆæå®Œæˆï¼")