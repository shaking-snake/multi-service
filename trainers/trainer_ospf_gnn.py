import torch
import torch.nn as nn
import torch.optim as optim
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch_geometric.loader import DataLoader 
from torch_geometric.data import Batch
from torch.utils.data import IterableDataset
from tqdm import tqdm
import os
import math
import numpy as np
import random
import argparse
import networkx as nx
import torch.nn.functional as F  # [å…³é”®ä¿®å¤] è¡¥å…¨ F å¯¼å…¥

# === å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ===
from MS.GNN.FiLMGnn import FiLMGnn 
from MS.Env.NetworkGenerator import TopologyGenerator, get_pyg_data_from_nx, DEFAULT_CONFIG
from MS.Env.MininetController import sample_path

# === è¾…åŠ©å‡½æ•° ===
def setup(rank, world_size):
  os.environ['MASTER_ADDR'] = 'localhost'
  os.environ['MASTER_PORT'] = '12355'
  
  # å…ˆè®¾ç½®å½“å‰è¿›ç¨‹å¯è§çš„ GPU
  torch.cuda.set_device(rank)
  
  # åˆå§‹åŒ–è¿›ç¨‹ç»„
  dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup():
    dist.destroy_process_group()

def generate_expert_label(G, S_node, D_node, edge_index):
  try:
    path_nodes = nx.dijkstra_path(G, S_node, D_node, weight='delay')
  except nx.NetworkXNoPath:
    return None

  path_edges = set()
  for i in range(len(path_nodes) - 1):
    u, v = path_nodes[i], path_nodes[i+1]
    path_edges.add((u, v))
    path_edges.add((v, u))

  num_total_edges = edge_index.shape[1]
  labels = torch.zeros(num_total_edges, dtype=torch.float)
  for i in range(num_total_edges):
    u, v = edge_index[0, i].item(), edge_index[1, i].item()
    if (u, v) in path_edges:
      labels[i] = 1.0
  return labels

# === é…ç½® ===
class Config:
  MIN_NODES_NUM = 15
  MAX_NODES_NUM = 30
  M_BA = 2
  MIN_BW = 2.0
  MAX_BW = 30.0
  MIN_DELAY = 1.0
  MAX_DELAY = 200.0
  MIN_LOSS = 0.0
  MAX_LOSS = 2.0

# === åŠ¨æ€æ•°æ®é›† ===
def generate_single_sample(topo_gen, config):
  while True:
    try:
      G_nx = topo_gen.generate_topology()
      S, D = topo_gen.select_source_destination()
      data, G_with_attrs = get_pyg_data_from_nx(G_nx, S, D, config)
      y = generate_expert_label(G_with_attrs, S, D, data.edge_index)
      if y is not None:
        data.y = y
        return data
    except Exception as e:
      print(e)
      continue

class DynamicGraphDataset(IterableDataset):
  def __init__(self, topo_gen, config, max_samples_per_epoch, rank, world_size):
    self.topo_gen = topo_gen
    self.config = config
    self.samples_per_gpu = int(math.ceil(max_samples_per_epoch / world_size))
    self.rank = rank
    self.world_size = world_size

  def __iter__(self):
    worker_info = torch.utils.data.get_worker_info()
    
    base_seed = torch.initial_seed() 
    if worker_info is not None:
      base_seed += worker_info.id
    
    # åŠ ä¸Š Rank åç§»
    unique_seed = base_seed + (self.rank * 10000)
    
    # [ä¿®æ­£ Numpy ç§å­æº¢å‡º] é™åˆ¶ç§å­èŒƒå›´
    random.seed(unique_seed)
    np.random.seed(unique_seed % (2**32 - 1)) 
    
    if worker_info is None:
      iter_count = self.samples_per_gpu
    else:
      per_worker = int(math.ceil(self.samples_per_gpu / float(worker_info.num_workers)))
      iter_count = min(per_worker, max(0, self.samples_per_gpu - worker_info.id * per_worker))
        
    for _ in range(iter_count):
      yield generate_single_sample(self.topo_gen, self.config)

# === æŸå¤±å‡½æ•° ===
class FocalLoss(nn.Module):
  def __init__(self, alpha=0.85, gamma=2.0, logits=True, reduce=True):
    super(FocalLoss, self).__init__()
    self.alpha = alpha
    self.gamma = gamma
    self.logits = logits
    self.reduce = reduce

  # åŠ å¼ºå¯¹æ­£æ ·æœ¬çš„æ³¨æ„--æé«˜å¬å›ç‡ï¼ŒåŠ å¼ºå¯¹ä¸ç¡®å®šæ ·æœ¬çš„å…³æ³¨--æ”»å…‹éš¾ç‚¹å›¾
  def forward(self, inputs, targets):
    if self.logits:
      BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
    else:
      BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')
    pt = torch.exp(-BCE_loss)
 
    alpha_t = self.alpha * targets  + (1 - self.alpha) * (1 - targets)
    F_loss = alpha_t * (1-pt)**self.gamma * BCE_loss * 10000

    if self.reduce: return torch.mean(F_loss)
    else: return F_loss

# === è®­ç»ƒä¸»é€»è¾‘ ===
def train_worker(rank, world_size):
  try:
    # 1. åˆå§‹åŒ–
    setup(rank, world_size)
    device = torch.device(f"cuda:{rank}")
    
    if rank == 0:
      print(f"ğŸš€ å¯åŠ¨ DDP è®­ç»ƒ | GPU: {world_size} | PID: {os.getpid()}")

    # 2. å‚æ•°
    EPOCHS = 6000          
    BATCH_SIZE = 512      
    TOTAL_SAMPLES = 6400   
    LEARNING_RATE = 5e-12
    
    # [ç¡®è®¤è¿™é‡Œçš„ç»´åº¦ä¸ NetworkGenerator ä¸€è‡´]
    NODE_FEAT_DIM = 10  # 10ç»´ (å« Buffer, ProcDelay)
    EDGE_FEAT_DIM = 5   # 5ç»´ (å« AvailBW)
    GNN_DIM = 256
    NUM_LAYERS = 6
    LOAD_PATH = "./trained_model/trained_gnn_ospf.pth"
    SAVE_PATH = "./trained_model/gnn_play_model.pth"
    LOAD_PATH = SAVE_PATH
    if rank == 0:
      os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)

    # 3. æ¨¡å‹
    model = FiLMGnn(
      node_feat_dim=NODE_FEAT_DIM, 
      gnn_dim=GNN_DIM, 
      edge_feat_dim=EDGE_FEAT_DIM, 
      num_layers=NUM_LAYERS
    ).to(device)
    if os.path.exists(LOAD_PATH):
      # æ˜ å°„åŠ è½½åˆ°å½“å‰ GPU è®¾å¤‡
      map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
      try:
        checkpoint = torch.load(LOAD_PATH, map_location=map_location)
        # åŠ è½½æƒé‡ (strict=False ä»¥é˜²å¾®å°ç»“æ„å˜åŠ¨)
        model.load_state_dict(checkpoint, strict=False)
        # if rank == 0:
        print(f"[Rank {rank}] æˆåŠŸåŠ è½½æ–­ç‚¹: {LOAD_PATH}")
      except Exception as e:
        if rank == 0:
          print(f"[Rank {rank}] åŠ è½½æ–­ç‚¹å¤±è´¥ (å°†é‡æ–°è®­ç»ƒ): {e}")
    else:
      if rank == 0:
        print(f"[ms] æœªå‘ç°æ–­ç‚¹ï¼Œå¼€å§‹å…¨æ–°è®­ç»ƒ...")
    # [DDP æ€§èƒ½ä¼˜åŒ–] find_unused_parameters=False æ¶ˆé™¤è­¦å‘Šå¹¶åŠ é€Ÿ
    model = DDP(model, device_ids=[rank], find_unused_parameters=False)

    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=1e-15)

    # å®šä¹‰é˜¶æ®µè¾¹ç•Œ
    loss_fn = FocalLoss(alpha=0.98, gamma=8.0)

    best_recall = 0.9989
    best_acc = 0.9
    topo_gen = TopologyGenerator(Config)

    # 4. å¾ªç¯
    for epoch in range(EPOCHS):
      model.train()
      
      dataset = DynamicGraphDataset(
        topo_gen, Config, 
        max_samples_per_epoch=TOTAL_SAMPLES, 
        rank=rank, 
        world_size=world_size
      )
      
      train_loader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        num_workers=4, 
        pin_memory=True
      )

      total_loss = 0.0
      total_acc = 0.0
      total_tp = 0.0      # True Positives (é¢„æµ‹ä¸º1ä¸”çœŸå®ä¸º1)
      total_real_p = 0.0  # Real Positives (çœŸå®ä¸º1çš„æ€»æ•°)
      num_batches = 0
      
      if rank == 0:
        pbar = tqdm(train_loader, leave=False)
      else:
        pbar = train_loader

      for batch in pbar:
        if rank == 0: pbar.set_description(f"[Epoch {epoch+1}]: ")

        batch = batch.to(device)
        optimizer.zero_grad()
        
        edge_logits = model(batch)
        y_true = batch.y
        
        loss = loss_fn(edge_logits, y_true)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        # ç»Ÿè®¡
        current_loss = loss.item()
        predicted = (edge_logits > 0.0).float()
        current_acc = (predicted == y_true).float().mean().item()
        tp = ((predicted == 1) & (y_true == 1)).sum().item()
        real_p = (y_true == 1).sum().item() 

        total_loss += current_loss
        total_acc += current_acc
        total_tp += tp
        total_real_p += real_p
        curr_rec = tp / (real_p + 1e-8)

        num_batches += 1

        # print(f"rank[{rank}] arrive at 1")

        if rank == 0:
          pbar.set_postfix({"Loss": f"{current_loss:.4f}", "Acc": f"{current_acc:.2%}", "Rec": f"{curr_rec:.2%}"})


      # print(f"rank[{rank}] arrive at 2")
      # æ±‡æ€» (Reduce)
      metrics = torch.tensor([total_loss, total_acc, num_batches, total_tp, total_real_p], device=device)
      dist.all_reduce(metrics, op=dist.ReduceOp.SUM)
      
      global_batches = metrics[2].item()
      global_tp = metrics[3].item()
      global_real_p = metrics[4].item()
      if global_batches > 0:
        avg_loss = metrics[0].item() / global_batches
        avg_acc = metrics[1].item() / global_batches
        avg_recall = global_tp / (global_real_p)
      else:
        avg_loss, avg_acc = 0.0, 0.0

      scheduler.step()

      # print(f"rank[{rank}] arrive at 3")
      # Rank 0 è´Ÿè´£æ›´æ–°å’Œä¿å­˜
      if rank == 0:
        current_lr = optimizer.param_groups[0]['lr']
        success_str = "N/A"

        if avg_recall > best_recall:
          best_recall = avg_recall
          torch.save(model.module.state_dict(), SAVE_PATH)
          
          # print(f"Saved Best: {best_acc:.2%}")
        try:
          # ç”Ÿæˆæµ‹è¯•å›¾
          model.eval()
          test_G = topo_gen.generate_topology()
          test_S, test_D = topo_gen.select_source_destination()
          test_data, _ = get_pyg_data_from_nx(test_G, test_S, test_D, DEFAULT_CONFIG)
          test_data = test_data.to(device)
          
          with torch.no_grad():
            val_logits = model.module(test_data)
          
          _, success = sample_path(
            val_logits, test_data.edge_index, 
            test_S, test_D, greedy=True
          )
          success_str = "Yes" if success else "No"

        except Exception as e:
          success_str = f"Err({str(e)[:20]})"
        finally:
          model.train()

      # æ˜¾å¼æŒ‡å®š device_ids
      if rank == 0: pbar.write(f"[Epoch {epoch+1}]Loss: {avg_loss:.4f}, Acc: {avg_acc:.2%}, LR: {current_lr:.2e}, Best: {best_recall:.2%}, Recall: {avg_recall:.2%}, Valid: {success_str}")
      # print(f"rank[{rank}] arrive")
      dist.barrier(device_ids=[rank])

  finally:
    cleanup()

# === ä¸»ç¨‹åº ===
if __name__ == "__main__":
  parser = argparse.ArgumentParser(description="GNN DDP Pretraining")
  parser.add_argument('--gpus', type=str, default='0,1,2,3', help='æŒ‡å®š GPU ID, e.g. "0,1"')
  args = parser.parse_args()

  os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus

  world_size = torch.cuda.device_count()
  if world_size < 1:
    print("[ms] éœ€è¦ GPU æ‰èƒ½è¿è¡Œæ­¤è„šæœ¬")
  else:
    mp.spawn(train_worker, args=(world_size,), nprocs=world_size, join=True)
